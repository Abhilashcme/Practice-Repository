{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "14_POS tagging & NER (Named Entity Recognition).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhilashcme/Practice-Repository/blob/master/14_POS_tagging_%26_NER_(Named_Entity_Recognition).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF6I-zdEXeQW"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1PMTi2Akvsbc3z28Pgi4zMMvI0oKUQGqw\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa5lnvyLVwUs"
      },
      "source": [
        "Tagging Part of Speech\n",
        "--\n",
        "Part of speech (POS) tagging is another crucial part of natural language\n",
        "processing that involves labeling the words with a part of speech such as\n",
        "noun, verb, adjective, etc. POS is the base for Named Entity Resolution,\n",
        "Sentiment Analysis, Question Answering, and Word Sense Disambiguation.\n",
        "\n",
        "Problem\n",
        "--\n",
        "Tagging the parts of speech for a sentence.\n",
        "\n",
        "Solution\n",
        "--\n",
        "There are 2 ways a tagger can be built.\n",
        "\n",
        "• Rule based - Rules created manually, which tag a word belonging to a particular POS.\n",
        "\n",
        "• Stochastic based - These algorithms capture the sequence of the words and tag the probability of the sequence using hidden Markov models.\n",
        "\n",
        "Again, NLTK has the best POS tagging module. nltk.pos_tag(word) is the\n",
        "function that will generate the POS tagging for any given word. Use for loop\n",
        "and generate POS for all the words present in the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhG_qZxqVwUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63bbc668-b2cc-42ce-8d23-523428eb764f"
      },
      "source": [
        "text = \"I love NLP and I will learn NLP in 2 month\"\n",
        "\n",
        "# NLTK for POS\n",
        "# Importing necessary packages and stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = sent_tokenize(text)\n",
        "\n",
        "#Generate tagging for all the tokens using loop\n",
        "for i in tokens:\n",
        " words = nltk.word_tokenize(i)\n",
        " words = [w for w in words if not w in stop_words]\n",
        "\n",
        "# POS-tagger.\n",
        "tags = nltk.pos_tag(words)\n",
        "\n",
        "print(tags)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[('I', 'PRP'), ('love', 'VBP'), ('NLP', 'NNP'), ('I', 'PRP'), ('learn', 'VBP'), ('NLP', 'RB'), ('2', 'CD'), ('month', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIKwUouQXGjg"
      },
      "source": [
        "Below are the short forms and explanation of POS tagging. \n",
        "\n",
        "**for example : The word “love” is VBP, which means verb; sing , dance.**\n",
        "\n",
        "• CC coordinating conjunction\n",
        "\n",
        "• CD cardinal digit\n",
        "\n",
        "• DT determiner\n",
        "\n",
        "• EX existential there (like: “there is” ... think of it like “there exists”)\n",
        "\n",
        "• FW foreign word\n",
        "\n",
        "• IN preposition/subordinating conjunction\n",
        "\n",
        "• JJ adjective ‘big’\n",
        "\n",
        "• JJR adjective, comparative ‘bigger’\n",
        "\n",
        "• JJS adjective, superlative ‘biggest’\n",
        "\n",
        "• LS list marker 1)  2)\n",
        "\n",
        "• MD modal could, will\n",
        "\n",
        "• NN noun, singular ‘desk’\n",
        "\n",
        "• NNS noun plural ‘desks’\n",
        "\n",
        "• NNP proper noun, singular ‘Harrison’\n",
        "\n",
        "• NNPS proper noun, plural ‘Americans’\n",
        "\n",
        "• PDT predeterminer ‘all the kids’\n",
        "\n",
        "• POS possessive ending parent’s\n",
        "\n",
        "• PRP personal pronoun I, he, she\n",
        "\n",
        "• PRP$ possessive pronoun my, his, hers\n",
        "\n",
        "• RB adverb very, silently\n",
        "\n",
        "• RBR adverb, comparative better\n",
        "\n",
        "• RBS adverb, superlative best\n",
        "\n",
        "• RP particle give up\n",
        "\n",
        "• TO to go ‘to’ the store\n",
        "\n",
        "• UH interjection\n",
        "\n",
        "• VB verb, base form take\n",
        "\n",
        "• VBD verb, past tense took\n",
        "\n",
        "• VBG verb, gerund/present participle taking\n",
        "\n",
        "• VBN verb, past participle taken\n",
        "\n",
        "• VBP verb, sing. present, non-3d take\n",
        "\n",
        "• VBZ verb, 3rd person sing. present takes\n",
        "\n",
        "• WDT wh-determiner which\n",
        "\n",
        "• WP wh-pronoun who, what\n",
        "\n",
        "• WP$ possessive wh-pronoun whose\n",
        "\n",
        "• WRB wh-adverb where, when\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhMkymkVVwU2"
      },
      "source": [
        "Extract Entities from Text\n",
        "--\n",
        "In this coding example, we are going to discuss how to identify and extract entities from the text, called Named Entity Recognition. \n",
        "\n",
        "There are multiple libraries to perform this task like NLTK chunker, StanfordNER, SpaCy, opennlp, and NeuroNER; and there are a lot of APIs also like WatsonNLU, AlchemyAPI, NERD, Google Cloud NLP API, and many more.\n",
        "\n",
        "Problem\n",
        "--\n",
        "You want to identify and extract entities from the text.\n",
        "\n",
        "Solution\n",
        "--\n",
        "The simplest way to do this is by using the ne_chunk from NLTK or SpaCy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDE2CO1vVwU3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "outputId": "2f3c30dd-2a4f-40e2-d3ab-79f3e5678c1d"
      },
      "source": [
        "sent = \"John is studying at Stanford University in California\"\n",
        "\n",
        "#import libraries\n",
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk import ne_chunk\n",
        "from nltk import word_tokenize\n",
        "\n",
        "#NER\n",
        "ne_chunk(nltk.pos_tag(word_tokenize(sent)), binary=False)\n",
        "# second binary parameter indicates whether Named entity is Person, \n",
        "# Org or GPE.\n",
        "# i.e binary=False would help you in classifying the NE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0m_canvas_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0mwidget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_to_treesegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_widget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/draw/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parent, **kw)\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \u001b[0;31m# If no parent was given, set up a top-level window.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NLTK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<Control-p>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2021\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('PERSON', [('John', 'NNP')]), ('is', 'VBZ'), ('studying', 'VBG'), ('at', 'IN'), Tree('ORGANIZATION', [('Stanford', 'NNP'), ('University', 'NNP')]), ('in', 'IN'), Tree('GPE', [('California', 'NNP')])])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKIUnFTKVwVB"
      },
      "source": [
        "Here **\"John\"** is tagged as **\"PERSON\"**\n",
        "\n",
        "**\"Stanford\"** as **\"ORGANIZATION\"**\n",
        "\n",
        "**\"California\"** as **\"GPE\"**. Geopolitical entity, i.e. countries, cities, states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C68oXu9oVwVD"
      },
      "source": [
        "Use Cases of NER Models\n",
        "--\n",
        "Named Entity Recognition has a wide range of applications in the field of Natural Language Processing and Information Retrieval. \n",
        "\n",
        "Few such examples have been listed below :\n",
        "\n",
        "1> <font color='green'><b>Automatically Summarizing Resumes</b></font> : One of the key challenges faced by the HR Department across companies is to evaluate a gigantic pile of resumes to shortlist candidates. To add to their burden, resumes of applicants are often excessively populated in detail, of which, most of the information is irrelevant to what the evaluator is seeking. With the aim of simplifying this process, through our NER model, we could facilitate evaluation of resumes at a quick glance, thereby simplifying the effort required in shortlisting candidates among a pile of resumes.\n",
        "\n",
        "2> <font color='green'><b>Optimizing Search Engine Algorithms</b></font> : To design a search engine algorithm, instead of searching for an entered query across the millions of articles and websites online, a more efficient approach would be to run an NER model on the articles once and store the entities associated with them permanently. The key tags in the search query can then be compared with the tags associated with the website articles for a quick and efficient search.\n",
        "\n",
        "3> <font color='green'><b>Powering Recommender Systems</b></font> : NER can be used in developing algorithms for recommender systems which automatically filter relevant content we might be interested in and accordingly guide us to discover related and unvisited relevant contents based on our previous behaviour. This may be achieved by extracting the entities associated with the content in our history or previous activity and comparing them with label assigned to other unseen content to filter relevant ones.\n",
        "\n",
        "4> <font color='green'><b>Simplifying Customer Support </b></font>: NER can be used in recognizing relevant entities in customer complaints and feedback such as Product specifications, department or company branch details, so that the feedback is classified accordingly and forwarded to the appropriate department responsible for the identified product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYA7ZRUUVwVE"
      },
      "source": [
        "Discussion Case study ( 20 - 30 mins )\n",
        "--\n",
        "Automatic Resume Summarization using NER\n",
        "\n",
        "> Each team of 2 participants is expected to scan through this link :\n",
        "https://towardsdatascience.com/a-review-of-named-entity-recognition-ner-using-automatic-summarization-of-resumes-5248a75de175\n",
        "> and be ready for a Q n A round with the trainer "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp8B9nOIVwVF"
      },
      "source": [
        "Recommended Reading - Home Assignment\n",
        "--\n",
        "https://medium.freecodecamp.org/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaPnOBdjVwVG"
      },
      "source": [
        "Doing NER Using SpaCy\n",
        "--\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC2I5v_lVwVH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8f30ef5-83e9-4a53-cb60-1332ed0328e3"
      },
      "source": [
        "# doing NER Using SpaCy\n",
        "!pip install spacy\n",
        "import spacy\n",
        "spacyEnglishObject = spacy.load('en')\n",
        "\n",
        "# Read/create a sentence\n",
        "doc = spacyEnglishObject(u'Apple is ready to launch new phone worth $10000 in New york time square ')\n",
        "for ent in doc.ents:\n",
        " print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n",
            "Apple 0 5 ORG\n",
            "10000 42 47 MONEY\n",
            "New york 51 59 GPE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iMDFK8IVwVL"
      },
      "source": [
        "According to the output, **`Apple`** is an organization, **`10000`** is money, and **`New York`** is place. The results are accurate and can be used for any NLP applications."
      ]
    }
  ]
}