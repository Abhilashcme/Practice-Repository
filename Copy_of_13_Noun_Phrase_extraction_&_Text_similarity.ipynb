{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Copy of 13_Noun Phrase extraction & Text similarity.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhilashcme/Practice-Repository/blob/master/Copy_of_13_Noun_Phrase_extraction_%26_Text_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c6RiCqHpw2O"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1sFLHXhRwvL0WxghpBs09SaTkFv2z7tL1\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_Z-6KCIoD_u"
      },
      "source": [
        "Noun Phrase extraction & Text similarity\n",
        "--\n",
        "\n",
        "\n",
        "Extracting Noun Phrases\n",
        "--\n",
        "Noun Phrase extraction is important when you want to analyze the <b>“who”</b> in a sentence. Let’s see an example below using <font color='green'><b>TextBlob.</b></font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kb9HmlAoD_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b03dc02-3e10-45b3-a2e5-e88a1c6ccec2"
      },
      "source": [
        "#Import libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "\n",
        "#Extract noun\n",
        "blob = TextBlob(\"Manali and Sana are learning natural language processing\")\n",
        "\n",
        "# we are printing the Nouns identified\n",
        "for np in blob.noun_phrases:\n",
        " print(np)\n",
        "\n",
        "# Tip : try making the first char of \"manali\"  and \"sana\" in small case.\n",
        "# Please remember : In English language , Proper Noun Shd always start with Upper Case char."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "manali\n",
            "sana\n",
            "natural language processing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spSBdBzaoD_4"
      },
      "source": [
        "Finding Similarity Between Texts\n",
        "--\n",
        "\n",
        "<font color='green'>In this coding example, we are going to discuss how to find the similarity between two documents or text.</font> There are many similarity metrics like Euclidian, cosine, Jaccard, etc. \n",
        "\n",
        "Applications of text similarity can be found in areas like spelling correction and data deduplication.\n",
        "\n",
        "Here are a few of the similarity measures:\n",
        "\n",
        "> **Cosine similarity**: Calculates the cosine of the angle between the two vectors.\n",
        "\n",
        "> **Jaccard similarity**: The score is calculated using the intersection or union of words. <br>\n",
        "<font color='green'>Jaccard Index = (the number in both sets) / (the number in either set) * 100. </font>\n",
        "\n",
        "<small>Below example taken from http://infolab.stanford.edu/~ullman/mmds/ch3.pdf </small>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1VGAlYXhu54-b2yhkXMDvsjk5IxIEMxNA\" />\n",
        "\n",
        "> **Levenshtein distance**: Minimal number of insertions, deletions, and replacements required for transforming string “a” into string “b.”\n",
        "\n",
        "> **Hamming distance**: Number of positions with the same symbol in both strings. But it can be defined only for strings with equal length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh14IKProD_5"
      },
      "source": [
        "Problem\n",
        "--\n",
        "You want to find the similarity between texts/documents.\n",
        "\n",
        "Solution\n",
        "--\n",
        "The simplest way to do this is by using **cosine similarity** from the sklearn library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfSHEC2doD_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e55958-f618-4206-b23d-cf1abb4449e6"
      },
      "source": [
        "documents = (\n",
        "\"I like NLP\",\n",
        "\"I am exploring NLP\",\n",
        "\"I am a beginner in NLP\",\n",
        "\"I want to learn NLP\",\n",
        "\"I like advanced NLP\"\n",
        ")\n",
        "\n",
        "# code to find the similarity.\n",
        "# Import libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#Compute tfidf\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)  \n",
        "# converts each document or sentence into a vector of 10 numbers\n",
        "\n",
        "tfidf_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-mmAgY_oD__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f960a236-5728-4cf4-bd68-3241bd7ab30f"
      },
      "source": [
        "#compute similarity of the first sentence with rest of the sentences\n",
        "cosine_similarity(tfidf_matrix[0],tfidf_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.17682765, 0.14284054, 0.13489366, 0.68374784]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3HlLs6hoEAE"
      },
      "source": [
        "**Observation** : If we clearly observe, the `first` sentence and `last` sentence have higher similarity compared to the rest of the sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpicBc2SzJEf"
      },
      "source": [
        "Phonetic Matching Algo's - An Intro\n",
        "--\n",
        "\n",
        "Ahead we would be testing different Phonetic or fuzzy matching Algos : Soundex , NYSIIS , DMETAPHONE and levenshtien distance using fuzzy and fuzzywuzzy package\n",
        "\n",
        "**`Motivation behind using Phonetic Matching Algo's`**\n",
        "\n",
        "Searching for a person's name in a database is a unique challenge. Depending on the source and age of the data, you may not be able to count on the <font color='red'>spelling of the name being correct, or even the same name being spelled the same way when it appears more than once. </font> Discrepancies between stored data and search terms may be introduced due to personal choice or cultural differences in spellings, homophones, transcription errors, illiteracy, or simply lack of standardized spellings during some time periods. These sorts of problems are especially prevalent in <b>transcriptions</b> of handwritten historical records used by historians, genealogists, and other researchers.\n",
        "\n",
        "A common way to solve the string-search problem is to look for values that are \"close\" to the same as the search target. Using a traditional fuzzy match algorithm to compute the closeness of two arbitrary strings is expensive, though, and it isn't appropriate for searching large data sets. A better solution is to compute hash values for entries in the database in advance, and several special hash algorithms have been created for this purpose. These phonetic hash algorithms allow you to compare two words or names based on how they sound, rather than the precise spelling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aIbT7_XoEAF"
      },
      "source": [
        "Phonetic matching\n",
        "--\n",
        "\n",
        "The next version of similarity checking is **`phonetic matching`**, which **`roughly`**\n",
        "matches the two words or sentences and also creates an alphanumeric string as an encoded version of the text or word. \n",
        "\n",
        "It is very useful for searching large text corpora, correcting spelling errors, and matching relevant names.\n",
        "\n",
        "**`DMetaphone`** and **`nysiis`** are two main phonetic algorithms used for this purpose. The simplest way to do this is by using the <font color='green'><b>fuzzy</b></font> library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpV3MT9_oEAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "346524ea-8e33-433b-fc07-27696fea4836"
      },
      "source": [
        "!pip install fuzzy\n",
        "import fuzzy    # Source -> https://pypi.org/project/Fuzzy/\n",
        "\n",
        "# some quick examples usage\n",
        "soundex = fuzzy.Soundex(4)\n",
        "#print(soundex('fuzzy'))    # --> will generate codec error on Python 3. Use Double Metaphone\n",
        "\n",
        "# Trying DMetaphone\n",
        "dmeta = fuzzy.DMetaphone()\n",
        "print(dmeta('language'))\n",
        "\n",
        "# Trying nysiis\n",
        "print(fuzzy.nysiis('Ankita'))\n",
        "print(fuzzy.nysiis('Ankeeta'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/b0/210f790e81e3c9f86a740f5384c758ad6c7bc1958332cf64263a9d3cf336/Fuzzy-1.2.2.tar.gz\n",
            "Building wheels for collected packages: fuzzy\n",
            "  Building wheel for fuzzy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fuzzy: filename=Fuzzy-1.2.2-cp37-cp37m-linux_x86_64.whl size=161711 sha256=07ed2df78684a34c5f347f91285f1e5f135f7721ab53babe68c32594c02e69fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/f7/14/b7e20855729780e85322529469b2d1eadfd940e83d981373cc\n",
            "Successfully built fuzzy\n",
            "Installing collected packages: fuzzy\n",
            "Successfully installed fuzzy-1.2.2\n",
            "[b'LNKJ', b'LNKK']\n",
            "ANCAT\n",
            "ANCAT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEpbg54gJ2YZ"
      },
      "source": [
        "#Understanding NYSIIS fuzzy match algo :\n",
        "\n",
        "Algorithms developed after Soundex use different encoding schemes, either building on Soundex by tweaking the lookup table or starting from scratch with their own rules. All of them process phonemes differently in an attempt to improve accuracy. For example, in the 1970s, the New York State Identification and Intelligence System (NYSIIS) algorithm was published.\n",
        "\n",
        "*NYSIIS was originally used by what is now the New York State Division of Criminal Justice Services to help identify people in their database.* It produces better results than Soundex because it takes special care to handle phonemes that occur in European and Hispanic surnames.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXMmk9UuKIJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a3166f5-fae2-478c-894f-df653082bb71"
      },
      "source": [
        "import fuzzy\n",
        "\n",
        "names = [ 'Catherine', 'Katherine', 'Katarina',\n",
        "          'Johnathan', 'Jonathan', 'John',\n",
        "          'Teresa', 'Theresa',\n",
        "          'Smith', 'Smyth',\n",
        "          'Jessica',\n",
        "          'Joshua',\n",
        "          ]\n",
        "\n",
        "for n in names:\n",
        "    print('%-10s' % n, fuzzy.nysiis(n))  # o/p has word and phoneme resp."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Catherine  CATARAN\n",
            "Katherine  CATARAN\n",
            "Katarina   CATARAN\n",
            "Johnathan  JANATAN\n",
            "Jonathan   JANATAN\n",
            "John       JAN\n",
            "Teresa     TARAS\n",
            "Theresa    TARAS\n",
            "Smith      SNATH\n",
            "Smyth      SNATH\n",
            "Jessica    JASAC\n",
            "Joshua     JAS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0Da7i1gKmyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d8f5e7-58a3-4bc0-dec0-3d883a7da5d2"
      },
      "source": [
        "import fuzzy\n",
        "\n",
        "names = [ 'Catherine', 'Katherine', 'Katarina',\n",
        "          'Johnathan', 'Jonathan', 'John',\n",
        "          'Teresa', 'Theresa',\n",
        "          'Smith', 'Smyth',\n",
        "          'Jessica',\n",
        "          'Joshua',\n",
        "          ]\n",
        "\n",
        "dmetaphone = fuzzy.DMetaphone(4)\n",
        "\n",
        "for n in names:\n",
        "    print('%-10s' % n, dmetaphone(n))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Catherine  [b'K0RN', b'KTRN']\n",
            "Katherine  [b'K0RN', b'KTRN']\n",
            "Katarina   [b'KTRN', None]\n",
            "Johnathan  [b'JN0N', b'ANTN']\n",
            "Jonathan   [b'JN0N', b'ANTN']\n",
            "John       [b'JN', b'AN']\n",
            "Teresa     [b'TRS', None]\n",
            "Theresa    [b'0RS', b'TRS']\n",
            "Smith      [b'SM0', b'XMT']\n",
            "Smyth      [b'SM0', b'XMT']\n",
            "Jessica    [b'JSK', b'ASK']\n",
            "Joshua     [b'JX', b'AX']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWq4nVcbKw9V"
      },
      "source": [
        "#Understanding the o/p of Double Metaphone \n",
        "\n",
        "The Metaphone algorithm is significantly more complicated than the others because it includes special rules for handling spelling inconsistencies and for looking at combinations of consonants in addition to some vowels. An updated version of the algorithm, called Double Metaphone, goes even further by adding rules for handling some spellings and pronunciations from languages other than English.\n",
        "\n",
        "In addition to having a broader set of encoding rules, Double Metaphone **generates two alternate hashes for each input word**. This gives the caller the ability to present search results with two levels of precision. In the results from the sample program, Catherine and Katherine have the same primary hash value. Their secondary hash value is the same as the primary hash for Katarina."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_nPOeQSLWOJ"
      },
      "source": [
        "#Levenshtein distance\n",
        "\n",
        "In information theory and computer science, the Levenshtein distance is a metric for measuring the amount of difference between two sequences (i.e. an edit distance). The Levenshtein distance between two strings is defined as the minimum number of edits needed to transform one string into the other, with the allowable edit operations being insertion, deletion, or substitution of a single character.\n",
        "\n",
        "\n",
        "<u>Example</u> <br>\n",
        "The Levenshtein distance between \"kitten\" and \"sitting\" is 3, since the following three edits change one into the other, and there isn't a way to do it with fewer than **three edits**:\n",
        "\n",
        "  1. kitten   sitten   (substitution of 'k' with 's')\n",
        "  \n",
        "  2. sitten   sittin   (substitution of 'e' with 'i')\n",
        "  \n",
        "  3. sittin   sitting   (insert 'g' at the end).\n",
        "\n",
        "\n",
        "\n",
        "**Install python-Levenshtein module as :**  <br>\n",
        "!pip install python-Levenshtein\n",
        "\n",
        "Source :\n",
        "https://pypi.org/project/python-Levenshtein/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmIJe3ICM-9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429c34fa-9a1a-4774-a917-8f11601f3de0"
      },
      "source": [
        "!pip install python-Levenshtein"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/dc/97f2b63ef0fa1fd78dcb7195aca577804f6b2b51e712516cc0e902a9a201/python-Levenshtein-0.12.2.tar.gz (50kB)\n",
            "\r\u001b[K     |██████▌                         | 10kB 16.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 20kB 22.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 30kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 40kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (54.0.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149821 sha256=0279e22500c0412bd8383ddd4496389df2fe3b8a2aa6d28398e5d73224d6da47\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/26/73/4b48503bac73f01cf18e52cd250947049a7f339e940c5df8fc\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCKKdVJ-NHvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "747ee3c7-1337-4e77-9019-1f4fad780092"
      },
      "source": [
        "import Levenshtein as lev\n",
        "Str1 = \"Apple Inc.\"\n",
        "Str2 = \"apple Inc\"\n",
        "Distance = lev.distance(Str1.lower(),Str2.lower()),\n",
        "print(Distance)\n",
        "Ratio = lev.ratio(Str1.lower(),Str2.lower())\n",
        "print(Ratio)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1,)\n",
            "0.9473684210526315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH30w2AfNN5t"
      },
      "source": [
        "#The FuzzyWuzzy Package\n",
        "This package may have a funny name, but it can be your best friend when the standard Levenshtein distance ratio of similarity between two strings falls short. So far the example that I have been using with \"Apple Inc.\" and \"apple Inc\" has been relatively simple. After all, there is just one full stop/period of difference if you turn both strings to lower case. \n",
        "\n",
        "However, what happens when something is spelled out of order? ***What happens when something has considerable spelling variation, but yet it refers to the same thing?*** That's where the <font color='green'>FuzzyWuzzy package</font> comes in since it has functions that allow our fuzzy matching scripts to handle these sorts of cases.\n",
        "\n",
        "Let's start simple. FuzzyWuzzy has, just like the Levenshtein package, a ratio function that computes the standard Levenshtein distance similarity ratio between two sequences. You can see an example below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y74qLPMBNehY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b36e49-69e9-41af-e4d2-e73eed87af5a"
      },
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XICt8UTnNsJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aacf4f96-7861-43c0-af1f-d37289cec0a9"
      },
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "Str1 = \"Apple Inc.\"\n",
        "Str2 = \"apple Inc\"\n",
        "Ratio = fuzz.ratio(Str1.lower(),Str2.lower())\n",
        "print(Ratio)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGS2Q4leNw_-"
      },
      "source": [
        "That ratio of similarity is the same as we expected given the other examples above. However, fuzzywuzzy has more powerful functions that allow us to deal with more complex situations such as **`substring matching`**. Here is an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Td8WYaRN2zU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc4ca331-81d5-4aa1-b055-521ffb10ff6d"
      },
      "source": [
        "Str1 = \"Los Angeles Lakers\"\n",
        "Str2 = \"Lakers\"\n",
        "\n",
        "Ratio = fuzz.ratio(Str1.lower(),Str2.lower())\n",
        "\n",
        "Partial_Ratio = fuzz.partial_ratio(Str1.lower(),Str2.lower())\n",
        "\n",
        "print(Ratio)\n",
        "\n",
        "print(Partial_Ratio)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTQAnUxcOCss"
      },
      "source": [
        "fuzz.partial_ratio() is capable of detecting that both strings are referring to the Lakers. Thus, it yields 100% similarity. The way this works is by using an \"optimal partial\" logic. In other words, ***if the short string has length k and the longer string has the length m, then the algorithm seeks the score of the best matching length-k substring.***\n",
        "\n",
        "Nevertheless, this approach is not foolproof. What happens when the strings comparison the same, but they are in a different order? Luckily for us, fuzzywuzzy has a solution. You can see the example below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTT2xZN1OXx6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a06bc58-a994-4d5e-c47c-664f73c375bc"
      },
      "source": [
        "Str1 = \"united states v. nixon\"\n",
        "Str2 = \"Nixon v. United States\"\n",
        "\n",
        "Ratio = fuzz.ratio(Str1.lower(),Str2.lower())\n",
        "Partial_Ratio = fuzz.partial_ratio(Str1.lower(),Str2.lower())\n",
        "Token_Sort_Ratio = fuzz.token_sort_ratio(Str1,Str2)\n",
        "\n",
        "print(Ratio)\n",
        "print(Partial_Ratio)\n",
        "print(Token_Sort_Ratio)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59\n",
            "74\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PCGYU_UOfoY"
      },
      "source": [
        "<font color='green'>The fuzz.token functions have an important advantage over ratio and partial_ratio.</font> They tokenize the strings and preprocess them by turning them to lower case and getting rid of punctuation. In the case of <font color='green'>fuzz.token_sort_ratio()</font>, the string tokens get sorted alphabetically and then joined together. After that, a simple fuzz.ratio() is applied to obtain the similarity percentage. This allows cases such as court cases in this example to be marked as being the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73AS_YRFOx9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08101b63-a30a-4350-e77e-f9d2e6322b71"
      },
      "source": [
        "# Still, what happens if these two strings are of widely differing lengths? \n",
        "# Thats where fuzz.token_set_ratio() comes in. \n",
        "# Here is an example:\n",
        "\n",
        "Str1 = \"The supreme court case of Nixon vs The United States\"\n",
        "Str2 = \"Nixon v. United States\"\n",
        "Ratio = fuzz.ratio(Str1.lower(),Str2.lower())\n",
        "Partial_Ratio = fuzz.partial_ratio(Str1.lower(),Str2.lower())\n",
        "Token_Sort_Ratio = fuzz.token_sort_ratio(Str1,Str2)\n",
        "Token_Set_Ratio = fuzz.token_set_ratio(Str1,Str2)\n",
        "print(Ratio)\n",
        "print(Partial_Ratio)\n",
        "print(Token_Sort_Ratio)\n",
        "print(Token_Set_Ratio)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57\n",
            "77\n",
            "58\n",
            "95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6ewcgQyPDea"
      },
      "source": [
        "<font color='red'><b>95% similarity is that magic? </b></font> No, it's just string preprocessing under the hood. In particular, fuzz.token_set_ratio() takes a more flexible approach than fuzz.token_sort_ratio(). Instead of just tokenizing the strings, sorting and then pasting the tokens back together, token_set_ratio performs a set operation that takes out the common tokens (the intersection) and then makes fuzz.ratio() pairwise comparisons between the following new strings:\n",
        "\n",
        "**s1 = Sorted_tokens_in_intersection**\n",
        "\n",
        "**s2 = Sorted_tokens_in_intersection + sorted_rest_of_str1_tokens**\n",
        "\n",
        "**s3 = Sorted_tokens_in_intersection + sorted_rest_of_str2_tokens**\n",
        "\n",
        "<font color='green'> <i>\n",
        "The logic behind these comparisons is that since Sorted_tokens_in_intersection is always the same, the score will tend to go up as these words make up a larger chunk of the original strings or the remaining tokens are closer to each other. </i></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyshJB6vPl5u"
      },
      "source": [
        "**`Finally`**, the fuzzywuzzy package has a module called process that allows you to calculate the string with the highest similarity out of a vector of strings. You can see how this works below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXer5a4EPtos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f53d88-0b28-401d-89db-6808f3a3bd8a"
      },
      "source": [
        "from fuzzywuzzy import process\n",
        "\n",
        "str2Match = \"apple inc\"\n",
        "strOptions = [\"apple park\", \"Apple Inc.\",\"apple incorporated\",\"iphone\"]\n",
        "\n",
        "Ratios = process.extract(str2Match,strOptions)\n",
        "\n",
        "print(Ratios)  # o/p is in sorted order of similarity index\n",
        "\n",
        "# You can also select the string with the highest matching percentage\n",
        "highest = process.extractOne(str2Match,strOptions)\n",
        "print(highest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Apple Inc.', 100), ('apple incorporated', 90), ('apple park', 67), ('iphone', 30)]\n",
            "('Apple Inc.', 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzme5hcFU9BP"
      },
      "source": [
        "**`Just in case`** :\n",
        "\n",
        "If you would like to see a practical application of fuzzywuzzy package in action : <br>\n",
        "https://towardsdatascience.com/natural-language-processing-for-fuzzy-string-matching-with-python-6632b7824c49"
      ]
    }
  ]
}